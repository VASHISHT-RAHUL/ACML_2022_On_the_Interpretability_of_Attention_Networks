{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "show_attend_tell_updated.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "NP8XNKmhSGzl"
      ],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "4zY0BW6yX4pn"
      },
      "source": [
        "!git clone https://github.com/salaniz/pycocoevalcap.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NP8XNKmhSGzl"
      },
      "source": [
        "# Download annotations\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vr3sQO2lSB4A"
      },
      "source": [
        "!pip install pickle5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yJ-GAFNcSBuU"
      },
      "source": [
        "import os\r\n",
        "import urllib.request\r\n",
        "os.makedirs('/content/opt' , exist_ok=True)\r\n",
        "os.chdir( '/content/opt' )\r\n",
        "!git clone 'https://github.com/cocodataset/cocoapi.git'\r\n",
        "\r\n",
        "\r\n",
        "os.chdir('/content/opt/cocoapi')\r\n",
        "\r\n",
        "# Download the annotation : \r\n",
        "annotations_trainval2014 = 'http://images.cocodataset.org/annotations/annotations_trainval2014.zip'\r\n",
        "image_info_test2014 = 'http://images.cocodataset.org/annotations/image_info_test2014.zip'\r\n",
        "\r\n",
        "\r\n",
        "urllib.request.urlretrieve(annotations_trainval2014 , filename = 'annotations_trainval2014.zip' )\r\n",
        "urllib.request.urlretrieve(image_info_test2014 , filename= 'image_info_test2014.zip' )\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k734wfkkTMVp"
      },
      "source": [
        "import zipfile\r\n",
        "with zipfile.ZipFile('annotations_trainval2014.zip' , 'r') as zip_ref:\r\n",
        "  zip_ref.extractall( '/content/opt/cocoapi'  )  \r\n",
        "\r\n",
        "try:\r\n",
        "  os.remove( 'annotations_trainval2014.zip' )\r\n",
        "  print('zip removed')\r\n",
        "except:\r\n",
        "  None\r\n",
        "\r\n",
        "with zipfile.ZipFile('image_info_test2014.zip' , 'r') as zip_ref:\r\n",
        "  zip_ref.extractall( '/content/opt/cocoapi'  )  \r\n",
        "\r\n",
        "try:\r\n",
        "  os.remove( 'image_info_test2014.zip' )\r\n",
        "  print('zip removed')\r\n",
        "except:\r\n",
        "  None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DVqil9nmStRG"
      },
      "source": [
        "# Download Images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kRxAN_KJSBhf"
      },
      "source": [
        "os.chdir('/content/opt/cocoapi')\r\n",
        "\r\n",
        "#train2014 = 'http://images.cocodataset.org/zips/train2014.zip'\r\n",
        "#test2014 = 'http://images.cocodataset.org/zips/test2014.zip'\r\n",
        "val2014 = 'http://images.cocodataset.org/zips/val2014.zip'\r\n",
        "\r\n",
        "#urllib.request.urlretrieve( train2014 , 'train2014' )\r\n",
        "#urllib.request.urlretrieve( test2014 , 'test2014' )\r\n",
        "urllib.request.urlretrieve( val2014 , 'val2014' )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nPZrPmgaS7rH"
      },
      "source": [
        "os.chdir('/content/opt/cocoapi')\r\n",
        "with zipfile.ZipFile( 'val2014' , 'r' ) as zip_ref:\r\n",
        "  zip_ref.extractall( 'images' )\r\n",
        "\r\n",
        "try:\r\n",
        "  os.remove( 'val2014' )\r\n",
        "  print('zip removed')\r\n",
        "except:\r\n",
        "  None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d379kr1_SLxm"
      },
      "source": [
        "# Code\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4B5Xqn0XW5IN"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gh84nMK0R16W"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "import math\n",
        "import random\n",
        "import tqdm\n",
        "import os\n",
        "import sys\n",
        "import glob\n",
        "import re\n",
        "import unicodedata\n",
        "from tqdm import tqdm\n",
        "\n",
        "import pickle\n",
        "    \n",
        "import itertools\n",
        "import time\n",
        "import json\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision import models\n",
        "\n",
        "\n",
        "\n",
        "import torchvision.datasets as dset\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader,Dataset\n",
        "from torch.nn import functional as F\n",
        "\n",
        "#sys.path.append('coco_eval/pycocoevalcap/')\n",
        "#sys.path.append('coco_eval/pycocoevalcap/bleu')\n",
        "#sys.path.append('coco_eval/pycocoevalcap/cider')\n",
        "from pycocotools.coco import COCO\n",
        "from pycocoevalcap.eval import COCOEvalCap\n",
        "\n",
        "\n",
        "from dictionary import Vocabulary,EOS_token,PAD_token,SOS_token,UNK_token\n",
        "\n",
        "\n",
        "dataset_path = '/content/opt/cocoapi/'\n",
        "\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device('cuda')\n",
        "else:\n",
        "    device = torch.device('cpu')\n",
        "    \n",
        "print('Device in Use: ',device)\n",
        "print('Device Properties: ',torch.cuda.get_device_properties(device))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R6YVB4PwR17I"
      },
      "source": [
        "#Original Model Implementation Details\n",
        "  # Encoder - VGG19 14×14×512 feature map of the fourth convolutional layer before max pooling. 196 × 512 \n",
        "  # mini-batch - 64\n",
        "  # stopping criterion - early stopping on BLEU score\n",
        "  # model selection - BLEU on our validation set\n",
        "  # vocabulary size - 10,000\n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "#We observed a breakdown in correlation between the validation set log-likelihood and BLEU in the later stages of \n",
        "#training during our experiments\n",
        "\n",
        "#Hyperparameters\n",
        "batch_size = 64\n",
        "val_batch_size = 61\n",
        "feat_size = 512\n",
        "feat_len = 196\n",
        "embedding_size = 512\n",
        "hidden_size = 512\n",
        "attn_size = 256\n",
        "#output_size = voc.num_words\n",
        "rnn_dropout = 0.5\n",
        "teacher_forcing_ratio = 0.5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xNGLK-J9R17O"
      },
      "source": [
        "#Utility functions\n",
        "\n",
        "# Turn a Unicode string to plain ASCII, thanks to\n",
        "# https://stackoverflow.com/a/518232/2809427\n",
        "def unicodeToAscii(s):\n",
        "    return ''.join(\n",
        "        c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn'\n",
        "    )\n",
        "\n",
        "# Lowercase, trim, and remove non-letter characters\n",
        "def normalizeString(s):\n",
        "    s = unicodeToAscii(s.lower().strip())\n",
        "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
        "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
        "    return s\n",
        "\n",
        "def target_tensor_to_caption(target):\n",
        "    gnd_trh = []\n",
        "    lend = target.size()[1]\n",
        "    for i in range(lend):\n",
        "        tmp = ' '.join(voc.index2word[x.item()] for x in targets[:,i])\n",
        "        gnd_trh.append(tmp)\n",
        "    return gnd_trh\n",
        "\n",
        "def maskNLLLoss(inp, target, mask):\n",
        "\n",
        "    nTotal = mask.sum()\n",
        "    crossEntropy = -torch.log(torch.gather(inp.squeeze(0), 1, target.view(-1, 1)).squeeze(1).float())\n",
        "    loss = crossEntropy.masked_select(mask).mean()\n",
        "    loss = loss.to(device)\n",
        "    return loss, nTotal.item()\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oIKW_vuOR17R"
      },
      "source": [
        "train_image_path = os.path.join(dataset_path,'images/train2014')\n",
        "val_image_path = os.path.join(dataset_path + 'images/val2014')\n",
        "#test_image_path = os.path.join(dataset_path + 'test2014')\n",
        "\n",
        "annotation_path = os.path.join(dataset_path + 'annotations')\n",
        "train_annotation_file = os.path.join(annotation_path,'captions_train2014.json')\n",
        "val_annotation_file = os.path.join(annotation_path,'captions_val2014.json')\n",
        "\n",
        "prediction_file_path= os.path.join('/content/','Predicted_Results')\n",
        "\n",
        "#Add image augmentation later\n",
        "data_transform = transforms.Compose([transforms.Resize((224,224)),transforms.ToTensor()])\n",
        "\n",
        "coco_train = dset.CocoCaptions(root=train_image_path,annFile=train_annotation_file,transform=data_transform)\n",
        "coco_val = dset.CocoCaptions(root=val_image_path,annFile=val_annotation_file,transform=data_transform) \n",
        "img,target = coco_train[200]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t4EdyGY1R17U"
      },
      "source": [
        "voc = Vocabulary('COCO_TRAIN')\n",
        "voc.load()\n",
        "voc.trim(min_count=5) # remove words having frequency less than min_count\n",
        "print('Vocabulary size :',voc.num_words)\n",
        "\n",
        "# aa_val = json.load(open(val_annotation_file))['annotations']\n",
        "# aa_train = json.load(open(train_annotation_file))['annotations']\n",
        "# train_cap2imgId = {}\n",
        "# val_cap2imgId = {}\n",
        "# for a in aa_val:\n",
        "#     val_cap2imgId[a['caption']]=a['image_id']\n",
        "    \n",
        "# for a in aa_train:\n",
        "#     train_cap2imgId[a['caption']]=a['image_id']\n",
        "    \n",
        "\n",
        "\n",
        "class COCO14Dataset(Dataset):\n",
        "    def __init__(self,coco,voc,transforms=None):\n",
        "        self.coco = coco\n",
        "        self.voc = voc\n",
        "        self.transforms = transforms\n",
        "    def __len__(self):\n",
        "        return len(self.coco)\n",
        "    def __getitem__(self,idx):\n",
        "        img,target = self.coco[idx]\n",
        "        ide = self.coco.ids[idx]\n",
        "        lbl = normalizeString(random.choice(target))\n",
        "        label = []\n",
        "        for s in lbl.split(' '):\n",
        "            if s in list(voc.word2index.keys()):\n",
        "                label.append(voc.word2index[s])\n",
        "            else:\n",
        "                label.append(UNK_token)\n",
        "        label = label +[EOS_token]\n",
        "        \n",
        "        return img, label,ide\n",
        "   \n",
        "\n",
        "train_dset = COCO14Dataset(coco_train,voc,transforms=data_transform)\n",
        "val_dset = COCO14Dataset(coco_val,voc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gbMURrxLR17Y"
      },
      "source": [
        "def collate_fn(batch):\n",
        "    data = [item[0] for item in batch]\n",
        "    images = torch.stack(data,0)\n",
        "    \n",
        "    ides = torch.tensor([item[2] for item in batch])\n",
        "    \n",
        "    label = [item[1] for item in batch]\n",
        "    max_target_len = max([len(indexes) for indexes in label])\n",
        "    padList = list(itertools.zip_longest(*label, fillvalue = 0))\n",
        "    lengths = torch.tensor([len(p) for p in label])\n",
        "    padVar = torch.LongTensor(padList)\n",
        "    \n",
        "    m = []\n",
        "    for i, seq in enumerate(padVar):\n",
        "        #m.append([])\n",
        "        tmp = []\n",
        "        for token in seq:\n",
        "            if token == 0:\n",
        "                tmp.append(int(0))\n",
        "            else:\n",
        "                tmp.append(1)\n",
        "        m.append(tmp)\n",
        "    m = torch.tensor(m)\n",
        "    \n",
        "    return images, padVar, m, max_target_len, ides\n",
        "\n",
        "train_loader=DataLoader(train_dset,batch_size = batch_size, num_workers = 8,shuffle = True,\n",
        "                    collate_fn = collate_fn, drop_last=True)\n",
        "\n",
        "val_loader = DataLoader(val_dset,batch_size = val_batch_size, num_workers = 8,shuffle = False,collate_fn = collate_fn,\n",
        "                     drop_last=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hPhIQZesR17c"
      },
      "source": [
        "dataiter = iter(train_loader)\n",
        "features, targets, mask, max_length,ides= dataiter.next()\n",
        "\n",
        "features.size(),targets.size(),mask.size(),ides.size()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_iAIObsbR17i"
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    \n",
        "    def __init__(self,batch_size):\n",
        "        super(Encoder,self).__init__()\n",
        "        base_model = models.vgg19(pretrained=True)\n",
        "        layers_to_use = list(base_model.features.children())[:29]\n",
        "        self.model = nn.Sequential(*layers_to_use)\n",
        "        \n",
        "    def forward(self,image_batch):\n",
        "        batch_size = image_batch.size()[0]\n",
        "        output = self.model(image_batch).view(batch_size,512,-1)\n",
        "        output = output.permute(0,2,1)\n",
        "        return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1N91s4NnR17k"
      },
      "source": [
        "class SpatialAttention(nn.Module):\n",
        "    def __init__(self,hidden_size, feat_size, bottleneck_size):\n",
        "        super(SpatialAttention,self).__init__()\n",
        "        '''\n",
        "        Spatial Attention module. It depends on previous hidden memory in the decoder(of shape hidden_size),\n",
        "        feature at the source side ( of shape(196,feat_size) ).  \n",
        "        at(s) = align(ht,hs)\n",
        "              = exp(score(ht,hs)) / Sum(exp(score(ht,hs')))  \n",
        "        where\n",
        "        score(ht,hs) = ht.t * hs                         (dot)\n",
        "                     = ht.t * Wa * hs                  (general)\n",
        "                     = va.t * tanh(Wa[ht;hs])           (concat)  \n",
        "        Here we have used concat formulae.\n",
        "        Argumets:\n",
        "          hidden_size : hidden memory size of decoder.\n",
        "          feat_size : feature size of each grid (annotation vector) at encoder side.\n",
        "          bottleneck_size : intermediate size.\n",
        "        '''\n",
        "        self.hidden_size = hidden_size\n",
        "        self.feat_size = feat_size\n",
        "        self.bottleneck_size = bottleneck_size\n",
        "        \n",
        "        self.decoder_projection = nn.Linear(hidden_size,bottleneck_size)\n",
        "        self.encoder_projection = nn.Linear(feat_size, bottleneck_size)\n",
        "        self.final_projection = nn.Linear(bottleneck_size,1)\n",
        "     \n",
        "    def forward(self,hidden,feats):\n",
        "        '''\n",
        "        shape of hidden (hidden_size)\n",
        "        shape of feats (196,feat_size)\n",
        "        '''\n",
        "        Wh = self.decoder_projection(hidden)  # (256)\n",
        "        Uv = self.encoder_projection(feats)   # (60,256)\n",
        "        #print(' Wh(hidden to bottleneck)  Uv(Feats to bottleneck)',Wh.size(),Uv.size())\n",
        "        Wh = Wh.unsqueeze(1).expand_as(Uv)\n",
        "        #print('Wh size  : ',Wh.size())\n",
        "        energies = self.final_projection(torch.tanh(Wh+Uv))\n",
        "        #print('energies : ',Uv.size())\n",
        "        weights = F.softmax(energies, dim=1)\n",
        "        \n",
        "        weighted_feats = feats *weights.expand_as(feats)\n",
        "        attn_feats = weighted_feats.sum(dim=1)\n",
        "        return attn_feats,weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yUqwKsciR17p"
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "    \n",
        "    def __init__(self, feat_size, feat_len, embedding_size, hidden_size, attn_size, output_size, rnn_dropout,\n",
        "                num_layers = 1, num_directions = 1):\n",
        "        super(Decoder, self).__init__()\n",
        "        '''\n",
        "        Decoder, Basically a language model.\n",
        "        \n",
        "        Arguments:\n",
        "        hidden_size : hidden memory size of LSTM/GRU\n",
        "        output_size : output size. Its same as the vocabulary size.\n",
        "        n_layers : \n",
        "        \n",
        "        '''\n",
        "\n",
        "        # Keep for reference\n",
        "        self.feat_size = feat_size\n",
        "        self.feat_len = feat_len\n",
        "        self.embedding_size = embedding_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.attn_size = attn_size\n",
        "        self.output_size = output_size\n",
        "        self.rnn_dropout = rnn_dropout\n",
        "        \n",
        "        self.num_layers = num_layers\n",
        "        self.num_directions = num_directions\n",
        "\n",
        "        # Define layers\n",
        "        self.embedding = nn.Embedding(self.output_size, self.embedding_size)\n",
        "        \n",
        "        self.attention = SpatialAttention(hidden_size = self.num_directions*self.hidden_size,\n",
        "                                          feat_size=self.feat_size,\n",
        "                                          bottleneck_size=self.attn_size)\n",
        "        \n",
        "        \n",
        "        self.rnn = nn.LSTM(self.embedding_size+self.feat_size, self.hidden_size,\n",
        "                           self.num_layers, dropout=self.rnn_dropout,batch_first=False, \n",
        "                          bidirectional=True if self.num_directions ==2 else False)\n",
        "        \n",
        "        self.out = nn.Linear(self.num_directions*self.hidden_size, self.output_size)\n",
        "\n",
        "    def get_last_hidden(self, hidden):\n",
        "        \n",
        "        last_hidden = hidden[0] if isinstance(hidden,tuple) else hidden\n",
        "        last_hidden = last_hidden.view(self.num_layers, self.num_directions,\n",
        "                                       last_hidden.size(1),last_hidden.size(2))\n",
        "        last_hidden = last_hidden.transpose(2,1).contiguous()\n",
        "        last_hidden = last_hidden.view(self.num_layers,last_hidden.size(1),\n",
        "                                       self.num_directions*last_hidden.size(3))\n",
        "        last_hidden = last_hidden[-1]\n",
        "        return last_hidden\n",
        "    \n",
        "    \n",
        "    def forward(self, inputs, hidden, feats):\n",
        "        # Note: we run this one step (word) at a time\n",
        "        # Get embedding of current input word\n",
        "        \n",
        "        # inputs -  (1,batch)\n",
        "        # hidden - (num_layers * num_directions, batch, hidden_size)\n",
        "        # feats - (batch,attention_length,annotation_vector_size) (32,196,512)\n",
        "        \n",
        "        #print('input  hidden  feats :',inputs.size(),hidden[0].size(),feats.size())\n",
        "        \n",
        "        embedded = self.embedding(inputs)\n",
        "        \n",
        "        last_hidden = hidden[0]\n",
        "        #print('embedded size :',embedded.size())\n",
        "        \n",
        "        feats, attn_weights = self.attention(last_hidden.squeeze(0),feats)\n",
        "\n",
        "        input_combined = torch.cat((embedded,feats.unsqueeze(0)),dim=2)\n",
        "        #print('input combined :',input_combined.size())\n",
        "\n",
        "        output, hidden = self.rnn(input_combined, hidden)\n",
        "\n",
        "        output = output.squeeze(0)\n",
        "        output = self.out(output)\n",
        "        output = F.softmax(output, dim = 1)\n",
        "        \n",
        "        return output, hidden, attn_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B6vnvknXR171"
      },
      "source": [
        "enc = Encoder(batch_size).to(device)\n",
        "dec = Decoder(feat_size,feat_len,embedding_size,hidden_size,attn_size,voc.num_words,rnn_dropout).to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z8lDNx3_R18B"
      },
      "source": [
        "class ShowAttendTell(nn.Module):\n",
        "    \n",
        "    def __init__(self,encoder,decoder,vocabulary,teacher_forcing_ratio, batch_size=batch_size,):\n",
        "        super(ShowAttendTell,self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.voc = vocabulary\n",
        "        self.batch_size = batch_size\n",
        "        self.enc_optimizer = optim.Adam(self.encoder.parameters(),lr=0.00001)\n",
        "        self.dec_optimizer = optim.Adam(self.decoder.parameters(),lr=0.001)\n",
        "        self.teacher_forcing_ratio = teacher_forcing_ratio\n",
        "        self.print_every = 400\n",
        "        \n",
        "    def update_hyperparam(self,encoder_lr,decoder_lr,teacher_forcing_ratio):\n",
        "        self.encoder_lr = encoder_lr\n",
        "        self.decoder_lr = decoder_lr\n",
        "        self.enc_optimizer = optim.Adam(self.encoder.parameters(),lr=self.encoder_lr)\n",
        "        self.dec_optimizer = optim.Adam(self.decoder.parameters(),lr=self.decoder_lr)\n",
        "        self.teacher_forcing_ratio = teacher_forcing_ratio\n",
        " \n",
        "        \n",
        "    def load(self,encoder_path = 'Save/VGG_encoder_10.pt',decoder_path='Save/VGG_decoder_10.pt'):\n",
        "        self.encoder = torch.load(encoder_path)\n",
        "        self.decoder = torch.load(decoder_path)\n",
        "        #self.encoder.load_state_dict(torch.load(encoder_path))\n",
        "        #self.decoder.load_state_dict(torch.load(decoder_path))\n",
        "        \n",
        "    def train_epoch(self,dataloader,clip=5):\n",
        "        \n",
        "        total_loss = 0\n",
        "        start_iteration = 1\n",
        "        print_loss = 0\n",
        "        iteration = 1\n",
        "        for data in dataloader:\n",
        "            features, targets, mask, max_length,_ = data\n",
        "            use_teacher_forcing = True if random.random() < self.teacher_forcing_ratio else False\n",
        "            loss = self.train_iter(features,targets,mask,max_length,use_teacher_forcing,clip)\n",
        "            print_loss += loss\n",
        "            total_loss += loss\n",
        "\n",
        "        # Print progress\n",
        "            if iteration % self.print_every == 0:\n",
        "                print_loss_avg = print_loss / self.print_every\n",
        "                print(\"Iteration: {}; Percent complete: {:.1f}%; Average loss: {:.4f}\".\n",
        "                format(iteration, iteration / len(dataloader) * 100, print_loss_avg))\n",
        "                print_loss = 0\n",
        "            \n",
        "            iteration += 1\n",
        "            \n",
        "        return total_loss/len(dataloader)\n",
        "            \n",
        "        \n",
        "    def train_iter(self,input_variable, target_variable, mask,max_target_len,use_teacher_forcing,clip=5):\n",
        "        \n",
        "        self.enc_optimizer.zero_grad()\n",
        "        self.dec_optimizer.zero_grad()\n",
        "        \n",
        "        loss = 0\n",
        "        print_losses = []\n",
        "        n_totals = 0\n",
        "        \n",
        "        input_variable = input_variable.to(device)\n",
        "        target_variable = target_variable.to(device)\n",
        "        mask = mask.byte().to(device)\n",
        "        \n",
        "        enc_output = self.encoder(input_variable)\n",
        "        decoder_input = torch.LongTensor([[SOS_token for _ in range(batch_size)]])\n",
        "        decoder_input = decoder_input.to(device)\n",
        "        decoder_hidden = (torch.zeros(1, batch_size, self.decoder.hidden_size).to(device),\n",
        "                  torch.zeros(1, batch_size, self.decoder.hidden_size).to(device))\n",
        "        \n",
        "        # Forward batch of sequences through decoder one time step at a time\n",
        "        if use_teacher_forcing:\n",
        "            for t in range(max_target_len):\n",
        "                decoder_output, decoder_hidden,_ = self.decoder(decoder_input, decoder_hidden,enc_output)\n",
        "                decoder_input = target_variable[t].view(1, -1)\n",
        "                mask_loss, nTotal = maskNLLLoss(decoder_output.unsqueeze(0), target_variable[t], mask[t])\n",
        "                loss += mask_loss\n",
        "                print_losses.append(mask_loss.item() * nTotal)\n",
        "                n_totals += nTotal\n",
        "        else:\n",
        "            for t in range(max_target_len):\n",
        "                #print('decoder hidden in sampling :',decoder_hidden.size())\n",
        "                decoder_output, decoder_hidden,_ = self.decoder(decoder_input, decoder_hidden,enc_output )\n",
        "                # No teacher forcing: next input is decoder's own current output\n",
        "                _, topi = decoder_output.squeeze(0).topk(1)\n",
        "                decoder_input = torch.LongTensor([[topi[i][0] for i in range(self.batch_size)]])\n",
        "                decoder_input = decoder_input.to(device)\n",
        "                #print('decoder input size in training :',decoder_input.size())\n",
        "                # Calculate and accumulate loss\n",
        "                mask_loss, nTotal = maskNLLLoss(decoder_output, target_variable[t], mask[t])\n",
        "                loss += mask_loss\n",
        "                print_losses.append(mask_loss.item() * nTotal)\n",
        "                n_totals += nTotal\n",
        "\n",
        "        # Perform backpropatation\n",
        "        loss.backward()\n",
        "\n",
        "        # Clip gradients: gradients are modified in place\n",
        "        _ = nn.utils.clip_grad_norm_(self.encoder.parameters(), clip)\n",
        "        _ = nn.utils.clip_grad_norm_(self.decoder.parameters(), clip)\n",
        "\n",
        "        # Adjust model weights\n",
        "        self.enc_optimizer.step()\n",
        "        self.dec_optimizer.step()\n",
        "\n",
        "        return sum(print_losses) / n_totals\n",
        "            \n",
        "        \n",
        "    def Greedy_Decoding(self,features,max_length=15):\n",
        "        enc_output = self.encoder(features)\n",
        "        batch_size = features.size()[0]\n",
        "        decoder_hidden = (torch.zeros(1, batch_size, self.decoder.hidden_size).to(device),\n",
        "                          torch.zeros(1, batch_size, self.decoder.hidden_size).to(device))\n",
        "        \n",
        "        decoder_input = torch.LongTensor([[SOS_token for _ in range(batch_size)]]).to(device)\n",
        "        #print('Initial size',decoder_input.size())\n",
        "        caption = []\n",
        "        attn_weights = []\n",
        "        for _ in range(max_length):\n",
        "            decoder_output, decoder_hidden, attn_values = self.decoder(decoder_input, decoder_hidden, enc_output)\n",
        "            #print(attn_values.shape)\n",
        "\n",
        "            #print('decoder hidden size :',decoder_hidden.size())\n",
        "            #print('decoder output size :',decoder_output.size())\n",
        "            _, topi = decoder_output.topk(1)\n",
        "            #print(topi)\n",
        "            #print('topi size',topi.size())\n",
        "            decoder_input = topi.permute(1,0).to(device)\n",
        "            caption.append(topi.squeeze(1).cpu())\n",
        "            attn_weights.append(attn_values[:,:,0].cpu())\n",
        "        caption = torch.stack(caption,0).permute(1,0)\n",
        "        attn_weights= torch.stack(attn_weights,0)\n",
        "        #print(attn_weights.size())\n",
        "\n",
        "        caps_text = []\n",
        "        for dta in caption:\n",
        "            tmp = []\n",
        "            for token in dta:\n",
        "                if token.item() not in self.voc.index2word.keys() or token.item()==2:\n",
        "                    pass\n",
        "                else:\n",
        "                    tmp.append(self.voc.index2word[token.item()])\n",
        "            tmp = ' '.join(x for x in tmp)\n",
        "            caps_text.append(tmp)\n",
        "        return caption,caps_text,attn_weights\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PMkUvYa-R18D"
      },
      "source": [
        "class Evaluator:\n",
        "    \n",
        "    def __init__(self,arch_name,prediction_filepath,reference_file,dataloader):\n",
        "        self.arch_name = arch_name\n",
        "        self.prediction_filepath = prediction_filepath\n",
        "        self.dataloader = dataloader\n",
        "        self.coco = COCO(reference_file)\n",
        "        self.scores = {}\n",
        "        self.bleu4 = 0.195\n",
        "    \n",
        "    def prediction_list(self,model):\n",
        "        result = []\n",
        "        ide_list = []\n",
        "        caption_list =[]\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            for data in tqdm(self.dataloader):\n",
        "                features, targets, mask, max_length,ides= data\n",
        "                cap,cap_txt,_ = model.Greedy_Decoding(features.to(device))\n",
        "                ide_list += list(ides.cpu().numpy())\n",
        "                caption_list += cap_txt\n",
        "        for a in zip(ide_list,caption_list):\n",
        "            result.append({'image_id':a[0].item(),'caption':a[1].strip()})      \n",
        "        return result\n",
        "    \n",
        "    def prediction_file_generation(self,result,prediction_filename):\n",
        "    \n",
        "        self.predicted_file = os.path.join(self.prediction_filepath,prediction_filename) \n",
        "        with open(self.predicted_file, 'w') as fp:\n",
        "            json.dump(result,fp)\n",
        "            \n",
        "    def evaluate(self,model,epoch):\n",
        "        prediction_filename = self.arch_name+str(epoch)+'.json'\n",
        "        result = self.prediction_list(model)\n",
        "        self.prediction_file_generation(result,prediction_filename)\n",
        "        \n",
        "        cocoRes = self.coco.loadRes(self.predicted_file)\n",
        "        cocoEval = COCOEvalCap(self.coco,cocoRes)\n",
        "        scores = cocoEval.evaluate()\n",
        "        self.scores[epoch] = scores\n",
        "        print(scores[0])\n",
        "        if scores[3] > self.bleu4:\n",
        "            self.bleu4 = scores[3]\n",
        "            self.save_model(model,epoch)\n",
        "        return scores\n",
        "    def save_model(self,model,epoch):\n",
        "        print('Better result saving models....')\n",
        "        encoder_filename = \"/content/drive/MyDrive/image_caption/\"+ 'encoder_'+str(epoch)+'.pt'\n",
        "        decoder_filename = \"/content/drive/MyDrive/image_caption/\"+ 'decoder_'+str(epoch)+'.pt'\n",
        "        model.train()\n",
        "        torch.save(model.encoder,encoder_filename)\n",
        "        torch.save(model.decoder,decoder_filename)\n",
        "        print(\"model saved\")\n",
        "        #torch.save(model.encoder,encoder_filename)\n",
        "        #torch.save(model.decoder,decoder_filename)\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F1F4CsnFR18E"
      },
      "source": [
        "\n",
        "model = ShowAttendTell(enc,dec,voc,teacher_forcing_ratio=teacher_forcing_ratio,batch_size=batch_size)\n",
        "model.load('/content/drive/MyDrive/image_caption/encoder_20.pt','/content/drive/MyDrive/image_caption/decoder_20.pt')\n",
        "\n",
        "val_evaluator = Evaluator('epoch_',prediction_file_path,val_annotation_file,val_loader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7djS-oylZRuv"
      },
      "source": [
        "val_evaluator.bleu4 = 0.2076133554243478\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XY_EGX7HR18F"
      },
      "source": [
        "# 1-10 : 1e-3, 0.5     14\n",
        "# 10-15 : 1e-3, 0.5    15\n",
        "# 15-25 : 1e-4, 0.4    15.5\n",
        "# 25-30 : 1e-3, 0.7    18\n",
        "# 30-35 : 1e-4, 0.8    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MhF9rWYMR18G"
      },
      "source": [
        "start= time.time()\n",
        "model.update_hyperparam(0.0000001,0.00001,0.85)\n",
        "model.train()\n",
        "k=21\n",
        "for epoch in range(3):\n",
        "    \n",
        "    loss = model.train_epoch(train_loader)\n",
        "    print(' Epoch :',epoch,' Loss :',loss)\n",
        "    scores = val_evaluator.evaluate(model,epoch+k)\n",
        "    print(scores)\n",
        "    model.train()\n",
        "end= time.time()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KQimEcqaR18I"
      },
      "source": [
        "model.train()\n",
        "torch.save(model.encoder,'/content/drive/MyDrive/image_caption/encoder_18.pt')\n",
        "torch.save(model.decoder,'/content/drive/MyDrive/image_caption/decoder_18.pt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IgXKADJxR18J"
      },
      "source": [
        "dataiter = iter(val_loader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T1nL4FPpR18K"
      },
      "source": [
        "features, targets, mask, max_length,ides= dataiter.next()\n",
        "\n",
        "features.size(),targets.size(),mask.size(),ides.size()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L3yKnjC6R18M"
      },
      "source": [
        "cap,cap_txt,attn_wts = model.Greedy_Decoding(features.to(device))\n",
        "cap_txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZOePpqSNR18O"
      },
      "source": [
        "\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k6auH_IYR18P"
      },
      "source": [
        "val_evaluator = Evaluator('VGG_epoch_',prediction_file_path,val_annotation_file,val_loader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JthmbpuiR18Q"
      },
      "source": [
        "val_evaluator.evaluate(model,30)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CLbvOiOqR18R"
      },
      "source": [
        "predicted_val_file = os.path.join(prediction_file_path,'epoch_30.json') "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_FAP5rjBR18T"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "80w8gqfFR18U"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i_AD6tkmR18V"
      },
      "source": [
        "#torch.save(model.encoder,'Save/VGG_encoder_25.pt')\n",
        "#torch.save(model.decoder,'Save/VGG_decoder_25.pt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GxLU1jAxR18W"
      },
      "source": [
        "# def result_list_maker(voc,dset,model):\n",
        "#     results = []\n",
        "#     for idx in tqdm(range(len(dset))):\n",
        "#         #print('Creating prediction file......')\n",
        "#         features = dset[idx][0].unsqueeze(0).to(device)\n",
        "#         cap,cap_text = model.Greedy_Decoding(features)\n",
        "#         pred = str.strip(str(*cap_text))\n",
        "#         results.append({'image_id':dset[idx][2],'caption':pred.split('EOS')[0].rstrip()})\n",
        "#     return results"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HwPuKvUQR18X"
      },
      "source": [
        "val_coco = COCO(val_annotation_file)\n",
        "val_cocoRes = val_coco.loadRes(predicted_val_file)\n",
        "val_cocoEval = COCOEvalCap(val_coco,val_cocoRes)\n",
        "val_score = val_cocoEval.evaluate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sSl8b5ytR18Y"
      },
      "source": [
        "val_score[0][1][3]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "irGOJ8NuR18Z"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}