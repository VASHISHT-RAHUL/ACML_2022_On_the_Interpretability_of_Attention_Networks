{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.login()\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append('/home/malt/Documents/Notebooks')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import *\n",
    "from train import *\n",
    "from generate_data import *\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm as tqdm\n",
    "from matplotlib import pyplot as plt \n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn import functional as F\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Create_Mosaic_data(desired_num,m,foreground_label,background_label,foreground_data,background_data,dataset=\"None\"):\n",
    "    \n",
    "    if dataset ==\"training\":\n",
    "        n_bg = 35000\n",
    "        n_fg = 15000\n",
    "    elif dataset == \"test\":\n",
    "        n_bg = 7000\n",
    "        n_fg = 3000\n",
    "        \n",
    "    mosaic_data =[]      # list of mosaic images, each mosaic image is saved as list of 9 images\n",
    "    fore_idx =[]                   # list of indexes at which foreground image is present in a mosaic image i.e from 0 to 9               \n",
    "    mosaic_label=[]                # label of mosaic image = foreground class present in that mosaic\n",
    "    list_set_labels = [] \n",
    "    for i in tqdm(range(desired_num)):\n",
    "        set_idx = set()\n",
    "        np.random.seed(i)\n",
    "        bg_idx = np.random.randint(0,n_bg,m-1)\n",
    "        set_idx = set(background_label[bg_idx].tolist())\n",
    "        fg_idx = np.random.randint(0,n_fg)\n",
    "        set_idx.add(foreground_label[fg_idx].item())\n",
    "        fg = np.random.randint(0,m)\n",
    "        fore_idx.append(fg)\n",
    "        image_list,label = create_mosaic_img(foreground_data,background_data,foreground_label,bg_idx,fg_idx,fg,m)\n",
    "        mosaic_data.append(image_list)\n",
    "        mosaic_label.append(label)\n",
    "        list_set_labels.append(set_idx)\n",
    "    print(\"Mosaic Data Created\")\n",
    "    return mosaic_data,mosaic_label,fore_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "fg1, fg2, fg3 = 0,1,2\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=10, shuffle=False)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=10, shuffle=False)\n",
    "\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "foreground_classes = {'plane', 'car', 'bird'}\n",
    "\n",
    "background_classes = {'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck'}\n",
    "\n",
    "\n",
    "dataiter = iter(trainloader)\n",
    "train_bg_data=[]\n",
    "train_bg_label=[]\n",
    "train_fg_data=[]\n",
    "train_fg_label=[]\n",
    "batch_size=10\n",
    "\n",
    "for i in tqdm(range(5000)):   #5000*batch_size = 50000 data points\n",
    "    images, labels = dataiter.next()\n",
    "    for j in range(batch_size):\n",
    "        if(classes[labels[j]] in background_classes):\n",
    "            img = images[j].tolist()\n",
    "            train_bg_data.append(img)\n",
    "            train_bg_label.append(labels[j])\n",
    "        else:\n",
    "            img = images[j].tolist()\n",
    "            train_fg_data.append(img)\n",
    "            train_fg_label.append(labels[j])\n",
    "            \n",
    "train_fg_data = torch.tensor(train_fg_data)\n",
    "train_fg_label = torch.tensor(train_fg_label)\n",
    "train_bg_data = torch.tensor(train_bg_data)\n",
    "train_bg_label = torch.tensor(train_bg_label)\n",
    "print(\"Train Foreground Background Data created\")\n",
    "\n",
    "\n",
    "m = 9\n",
    "desired_num = 40000\n",
    "\n",
    "train_mosaic_data,train_mosaic_label,train_fore_idx = Create_Mosaic_data(desired_num,m,train_fg_label,\n",
    "                                     train_bg_label,train_fg_data,train_bg_data,\"training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MosaicDataset(Dataset):\n",
    "  \"\"\"MosaicDataset dataset.\"\"\"\n",
    "\n",
    "  def __init__(self, mosaic_list_of_images, mosaic_label, fore_idx):\n",
    "    \"\"\"\n",
    "      Args:\n",
    "        csv_file (string): Path to the csv file with annotations.\n",
    "        root_dir (string): Directory with all the images.\n",
    "        transform (callable, optional): Optional transform to be applied\n",
    "            on a sample.\n",
    "    \"\"\"\n",
    "    self.mosaic = mosaic_list_of_images\n",
    "    self.label = mosaic_label\n",
    "    self.fore_idx = fore_idx\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.label)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    return self.mosaic[idx] , self.label[idx], self.fore_idx[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = 256\n",
    "tr = 30000\n",
    "msd = MosaicDataset(train_mosaic_data[0:tr], train_mosaic_label[0:tr] , train_fore_idx[0:tr])\n",
    "train_loader = DataLoader( msd,batch_size= batch ,shuffle=False)\n",
    "\n",
    "batch = 256\n",
    "msd1 = MosaicDataset(train_mosaic_data[tr:], train_mosaic_label[tr:] , train_fore_idx[tr:])\n",
    "test_loader = DataLoader( msd1,batch_size= batch ,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_analysis(data_loader,focus,classification,dataset=\"None\"):\n",
    "    ftpt_1,ffpt_1,ftpf_1,ffpf_1,accuracy_1 = evaluation_method_1(data_loader,focus,classification)\n",
    "    ftpt_2,ffpt_2,ftpf_2,ffpf_2,accuracy_2 = evaluation_method_2(data_loader,focus,classification)\n",
    "    ftpt_3,ffpt_3,ftpf_3,ffpf_3,accuracy_3 = evaluation_method_3(data_loader,focus,classification)\n",
    "    \n",
    "    print(str(dataset)+\"_Evaluation Method 1\")\n",
    "    print(\"*\"*60)\n",
    "    print(\"FTPT\",ftpt_1)\n",
    "    print(\"FFPT\",ffpt_1)\n",
    "    print(\"FTPF\",ftpf_1)\n",
    "    print(\"FFPF\",ffpf_1)\n",
    "    print(\"Accuracy\",accuracy_1)\n",
    "    \n",
    "    print(str(dataset)+\"_Evaluation Method 2\")\n",
    "    print(\"*\"*60)\n",
    "    print(\"FTPT\",ftpt_2)\n",
    "    print(\"FFPT\",ffpt_2)\n",
    "    print(\"FTPF\",ftpf_2)\n",
    "    print(\"FFPF\",ffpf_2)\n",
    "    print(\"Accuracy\",accuracy_2)\n",
    "    \n",
    "    print(str(dataset)+\"_Evaluation Method 3\")\n",
    "    print(\"*\"*60)\n",
    "    print(\"FTPT\",ftpt_3)\n",
    "    print(\"FFPT\",ffpt_3)\n",
    "    print(\"FTPF\",ftpf_3)\n",
    "    print(\"FFPF\",ffpf_3)\n",
    "    print(\"Accuracy\",accuracy_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Focus_6(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(Focus_6, self).__init__()\n",
    "    self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=0,bias=False)\n",
    "    self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=0,bias=False)\n",
    "    self.conv3 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=0,bias=False)\n",
    "    self.conv4 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=0,bias=False)\n",
    "    self.conv5 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=0,bias=False)\n",
    "    self.conv6 = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1,bias=False)\n",
    "    self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "    self.batch_norm1 = nn.BatchNorm2d(32,track_running_stats=False)\n",
    "    self.batch_norm2 = nn.BatchNorm2d(64,track_running_stats=False)\n",
    "    self.batch_norm3 = nn.BatchNorm2d(256,track_running_stats=False)\n",
    "    self.dropout1 = nn.Dropout2d(p=0.05)\n",
    "    self.dropout2 = nn.Dropout2d(p=0.1)\n",
    "    self.fc1 = nn.Linear(256,64,bias=False)\n",
    "    self.fc2 = nn.Linear(64, 32,bias=False)\n",
    "    self.fc3 = nn.Linear(32, 10,bias=False)\n",
    "    self.fc4 = nn.Linear(10, 1,bias=False)\n",
    "\n",
    "    torch.nn.init.xavier_normal_(self.conv1.weight)\n",
    "    torch.nn.init.xavier_normal_(self.conv2.weight)\n",
    "    torch.nn.init.xavier_normal_(self.conv3.weight)\n",
    "    torch.nn.init.xavier_normal_(self.conv4.weight)\n",
    "    torch.nn.init.xavier_normal_(self.conv5.weight)\n",
    "    torch.nn.init.xavier_normal_(self.conv6.weight)\n",
    "\n",
    "    torch.nn.init.xavier_normal_(self.fc1.weight)\n",
    "    torch.nn.init.xavier_normal_(self.fc2.weight)\n",
    "    torch.nn.init.xavier_normal_(self.fc3.weight)\n",
    "    torch.nn.init.xavier_normal_(self.fc4.weight)\n",
    "\n",
    "  def forward(self,z):  #y is avg image #z batch of list of 9 images\n",
    "    batch = z.size(0)\n",
    "    patches = z.size(1)\n",
    "    z  =torch.reshape(z,(batch*patches,3,32,32))\n",
    "    alpha,features = self.helper(z)\n",
    "    \n",
    "    alpha = torch.reshape(alpha,(batch,patches))\n",
    "    #features = torch.reshape(features,(batch,patches,features.shape[1],features.shape[2],features.shape[3]))\n",
    "    return alpha,features #alpha, log_alpha, avg_data\n",
    "    \n",
    "  def helper(self, x):\n",
    "    #x1 = x\n",
    "    #x1 =x\n",
    "    x = self.conv1(x)\n",
    "    x = F.relu(self.batch_norm1(x))\n",
    "\n",
    "    x = (F.relu(self.conv2(x)))\n",
    "    x = self.pool(x)\n",
    "    \n",
    "    x = self.conv3(x)\n",
    "    x = F.relu(self.batch_norm2(x))\n",
    "\n",
    "    x = (F.relu(self.conv4(x)))\n",
    "    x = self.pool(x)\n",
    "    x = self.dropout1(x)\n",
    "\n",
    "    x = self.conv5(x)\n",
    "    \n",
    "    x = F.relu(self.batch_norm3(x))\n",
    "\n",
    "    x = self.conv6(x)\n",
    "    x1 = F.tanh(x)\n",
    "    x = F.relu(x)\n",
    "    x = self.pool(x)\n",
    "\n",
    "    x = x.view(x.size(0), -1)\n",
    "\n",
    "    x = self.dropout2(x)\n",
    "    x = F.relu(self.fc1(x))\n",
    "    x = F.relu(self.fc2(x))\n",
    "    x = self.dropout2(x)\n",
    "    x = F.relu(self.fc3(x))\n",
    "    x = self.fc4(x)\n",
    "    x = x[:,0] \n",
    "    #print(x.shape)\n",
    "    return x,x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classification_6(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(Classification_6, self).__init__()\n",
    "    self.conv1 = nn.Conv2d(in_channels=256, out_channels=128, kernel_size=3, padding=1)\n",
    "    self.conv2 = nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, padding=1)\n",
    "    self.conv3 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1)\n",
    "    self.conv4 = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1)\n",
    "    self.conv5 = nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, padding=1)\n",
    "    self.conv6 = nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1)\n",
    "    self.pool = nn.MaxPool2d(kernel_size=2, stride=2,padding=1)\n",
    "    self.batch_norm1 = nn.BatchNorm2d(128,track_running_stats=False)\n",
    "    self.batch_norm2 = nn.BatchNorm2d(256,track_running_stats=False)\n",
    "    self.batch_norm3 = nn.BatchNorm2d(512,track_running_stats=False)\n",
    "    self.dropout1 = nn.Dropout2d(p=0.05)\n",
    "    self.dropout2 = nn.Dropout2d(p=0.1)\n",
    "    self.global_average_pooling = nn.AvgPool2d(kernel_size=2)\n",
    "    self.fc1 = nn.Linear(512,128)\n",
    "    # self.fc2 = nn.Linear(128, 64)\n",
    "    # self.fc3 = nn.Linear(64, 10)\n",
    "    self.fc2 = nn.Linear(128, 3)\n",
    "\n",
    "    torch.nn.init.xavier_normal_(self.conv1.weight)\n",
    "    torch.nn.init.xavier_normal_(self.conv2.weight)\n",
    "    torch.nn.init.xavier_normal_(self.conv3.weight)\n",
    "    torch.nn.init.xavier_normal_(self.conv4.weight)\n",
    "    torch.nn.init.xavier_normal_(self.conv5.weight)\n",
    "    torch.nn.init.xavier_normal_(self.conv6.weight)\n",
    "\n",
    "    torch.nn.init.zeros_(self.conv1.bias)\n",
    "    torch.nn.init.zeros_(self.conv2.bias)\n",
    "    torch.nn.init.zeros_(self.conv3.bias)\n",
    "    torch.nn.init.zeros_(self.conv4.bias)\n",
    "    torch.nn.init.zeros_(self.conv5.bias)\n",
    "    torch.nn.init.zeros_(self.conv6.bias)\n",
    "\n",
    "\n",
    "    torch.nn.init.xavier_normal_(self.fc1.weight)\n",
    "    torch.nn.init.xavier_normal_(self.fc2.weight)\n",
    "    torch.nn.init.zeros_(self.fc1.bias)\n",
    "    torch.nn.init.zeros_(self.fc2.bias)\n",
    "\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.conv1(x)\n",
    "    x = F.relu(self.batch_norm1(x))\n",
    "\n",
    "    x = (F.relu(self.conv2(x)))\n",
    "    x = self.pool(x)\n",
    "    \n",
    "    x = self.conv3(x)\n",
    "    x = F.relu(self.batch_norm2(x))\n",
    "\n",
    "    x = (F.relu(self.conv4(x)))\n",
    "    x = self.pool(x)\n",
    "    x = self.dropout1(x)\n",
    "\n",
    "    x = self.conv5(x)\n",
    "    x = F.relu(self.batch_norm3(x))\n",
    "\n",
    "    x = (F.relu(self.conv6(x)))\n",
    "    x = self.pool(x)\n",
    "    #print(x.shape)\n",
    "    x = self.global_average_pooling(x)\n",
    "    x = x.squeeze()\n",
    "    #x = x.view(x.size(0), -1)\n",
    "    #print(x.shape)\n",
    "    x = self.dropout2(x)\n",
    "    x = F.relu(self.fc1(x))\n",
    "    #x = F.relu(self.fc2(x))\n",
    "    #x = self.dropout2(x)\n",
    "    #x = F.relu(self.fc3(x))\n",
    "    x = self.fc2(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model1(data,focus,classification,focus_optimizer,classification_optimizer,Criterion):\n",
    "    images,labels,fore_idx = data\n",
    "    images = images.float()\n",
    "    batch = images.size(0)\n",
    "    patches = images.size(1)\n",
    "    images = images.float()\n",
    "    images,labels = images.to(device),labels.to(device)\n",
    "            \n",
    "    focus_optimizer.zero_grad()\n",
    "    classification_optimizer.zero_grad()\n",
    "    alpha,features = focus(images)\n",
    "    #print(\"Flag 1\",alpha.shape,features.shape)\n",
    "    alphas = torch.softmax(alpha,dim=1)\n",
    "\n",
    "    outputs = classification(features)\n",
    "    loss = my_cross_entropy(outputs,labels,alphas,Criterion)\n",
    "            \n",
    "    loss.backward()\n",
    "    focus_optimizer.step()\n",
    "    classification_optimizer.step()\n",
    "        \n",
    "    return focus,classification,focus_optimizer,classification_optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(focus,classification,dataloader,dataset=\"train\"):\n",
    "    focus.eval()\n",
    "    classification.eval()\n",
    "    alphas = []\n",
    "    pred = []\n",
    "    fidices = []\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(dataloader, 0):\n",
    "            inputs, labels,fidx = data\n",
    "            inputs, labels = inputs.to(\"cuda\"),labels.to(\"cuda\")\n",
    "            alpha, avg_images = focus(inputs)\n",
    "            outputs = classification(avg_images)\n",
    "            alpha = torch.softmax(alpha,dim=1)\n",
    "            alphas.append(alpha.cpu().numpy())\n",
    "\n",
    "        alphas = np.concatenate(alphas,axis=0)\n",
    "\n",
    "\n",
    "        # value>0.01 here sum over all data points is returned to take average divide by number of data points\n",
    "        sparsity_val = np.sum(np.sum(alphas>0.01,axis=1))\n",
    "\n",
    "\n",
    "        # simplex distance here sum over all data points is returned to take average divide by number of data points\n",
    "        argmax_index = np.argmax(alphas,axis=1)\n",
    "        simplex_pt = np.zeros(alphas.shape)\n",
    "        simplex_pt[np.arange(argmax_index.size),argmax_index] = 1\n",
    "\n",
    "        shortest_distance_simplex = np.sum(np.sqrt(np.sum((alphas-simplex_pt)**2,axis=1))) \n",
    "\n",
    "        # entropy here sum over all data points is returned to take average divide by number of data points\n",
    "        entropy = np.sum(np.nansum(-alphas*np.log2(alphas),axis=1))\n",
    "    if dataset == \"train\":\n",
    "        val =30000\n",
    "    else:\n",
    "        val = 10000\n",
    "    print(\"dataset \"+dataset)\n",
    "    print(sparsity_val/val,shortest_distance_simplex/val,entropy/val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method 1\n",
    "def evaluation_method_1(dataloader,focus,classification):\n",
    "    \"\"\"\n",
    "    returns \\sigma_k(g(x_j*)) j* is the argmax_j(\\sigma_j(XU))\n",
    "    \"\"\"\n",
    "    predicted_indexes = []\n",
    "    foreground_index_list = []\n",
    "    prediction_list = []\n",
    "    labels_list = []\n",
    "    focus.eval()\n",
    "    classification.eval()\n",
    "    with torch.no_grad():\n",
    "        for j,data in enumerate(dataloader):\n",
    "            images,labels,foreground_index = data\n",
    "            images = images.float()\n",
    "            images = images.to(device)\n",
    "            foreground_index_list.append(foreground_index.numpy())\n",
    "            labels_list.append(labels.numpy())\n",
    "            batch = images.size(0)\n",
    "            scores, features = focus(images)\n",
    "            \n",
    "            if len(scores.shape)>2:\n",
    "                indexes = torch.argmax(F.softmax(scores,dim=1),dim=1).cpu().numpy()[:,0]\n",
    "            else:\n",
    "                indexes = torch.argmax(F.softmax(scores,dim=1),dim=1).cpu().numpy()\n",
    "            predicted_indexes.append(indexes)\n",
    "            \n",
    "            features = features.reshape(batch,patches,256,3,3)\n",
    "            features = features[np.arange(batch),indexes,:]\n",
    "            #print(features.shape)\n",
    "            outputs = F.softmax(classification(features),dim=1)\n",
    "            prediction = torch.argmax(outputs,dim=1)\n",
    "            prediction_list.append(prediction.cpu().numpy())\n",
    "\n",
    "    predicted_indexes = np.concatenate(predicted_indexes,axis=0)\n",
    "    foreground_index_list = np.concatenate(foreground_index_list,axis=0)\n",
    "    prediction_list = np.concatenate(prediction_list,axis=0)\n",
    "    labels_list = np.concatenate(labels_list,axis=0)\n",
    "    \n",
    "    #print(predicted_indexes.shape,foreground_index_list.shape)\n",
    "\n",
    "    ftpt = (np.sum(np.logical_and(predicted_indexes == foreground_index_list,\n",
    "                                 prediction_list == labels_list),axis=0).item()/len(foreground_index_list))*100\n",
    "    \n",
    "    ffpt  = (np.sum(np.logical_and(predicted_indexes != foreground_index_list,\n",
    "                                 prediction_list == labels_list),axis=0).item()/len(foreground_index_list))*100\n",
    "    \n",
    "    ftpf  = (np.sum(np.logical_and(predicted_indexes == foreground_index_list,\n",
    "                                 prediction_list != labels_list),axis=0).item()/len(foreground_index_list))*100\n",
    "    \n",
    "    ffpf  = (np.sum(np.logical_and(predicted_indexes != foreground_index_list,\n",
    "                                 prediction_list != labels_list),axis=0).item()/len(foreground_index_list))*100\n",
    "#     focus_true = (np.sum(predicted_indexes == foreground_index_list,axis=0).item()/\n",
    "#                         len(foreground_index_list))*100\n",
    "    accuracy = (np.sum(prediction_list == labels_list,axis=0)/len(labels_list) )*100\n",
    "    \n",
    "    return ftpt,ffpt,ftpf,ffpf,accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation_method_2(dataloader,focus,classification):\n",
    "    \"\"\"\n",
    "    returns \\sum_j(\\alpha_j * \\sigma_k(g(x_j)) \n",
    "    \"\"\"\n",
    "    predicted_indexes = []\n",
    "    foreground_index_list = []\n",
    "    prediction_list = []\n",
    "    labels_list = []\n",
    "    focus.eval()\n",
    "    classification.eval()\n",
    "    with torch.no_grad():\n",
    "        for j,data in enumerate(dataloader):\n",
    "            images,labels,foreground_index = data\n",
    "            images = images.float()\n",
    "            images = images.to(device)\n",
    "            batch = images.size(0)\n",
    "            patches = images.size(1)\n",
    "            foreground_index_list.append(foreground_index.numpy())\n",
    "            labels_list.append(labels.numpy())\n",
    "            batch = images.size(0)\n",
    "            alpha,features = focus(images)\n",
    "            focus_outputs = F.softmax(alpha,dim=1)\n",
    "            if len(focus_outputs.shape)>2:\n",
    "                focus_outputs = focus_outputs[:,:,0]\n",
    "            indexes = torch.argmax(focus_outputs,dim=1).cpu().numpy()\n",
    "            predicted_indexes.append(indexes)\n",
    "            \n",
    "            if len(images.shape)>3:\n",
    "                features = features.reshape(batch*patches,256,3,3)\n",
    " \n",
    "            classification_outputs = F.softmax(classification(features),dim=1)\n",
    "            n_classes = classification_outputs.size(1)\n",
    "            classification_outputs = classification_outputs.reshape(batch,patches,n_classes)\n",
    "\n",
    "            #print(classification_outputs.shape,focus_outputs.shape)\n",
    "            if len(images.shape)>3:\n",
    "                focus_outputs = focus_outputs[:,:,None]\n",
    "            else:\n",
    "                focus_outputs = focus_outputs[:,:,None]\n",
    "            prediction = torch.argmax(torch.sum(focus_outputs*classification_outputs,dim=1),dim=1)\n",
    "            \n",
    "           \n",
    "            prediction_list.append(prediction.cpu().numpy())\n",
    "\n",
    "    predicted_indexes = np.concatenate(predicted_indexes,axis=0)\n",
    "    foreground_index_list = np.concatenate(foreground_index_list,axis=0)\n",
    "    prediction_list = np.concatenate(prediction_list,axis=0)\n",
    "    labels_list = np.concatenate(labels_list,axis=0)\n",
    "    print(prediction_list.shape)\n",
    "\n",
    "    ftpt = (np.sum(np.logical_and(predicted_indexes == foreground_index_list,\n",
    "                                 prediction_list == labels_list),axis=0).item()/len(foreground_index_list))*100\n",
    "    \n",
    "    ffpt  = (np.sum(np.logical_and(predicted_indexes != foreground_index_list,\n",
    "                                 prediction_list == labels_list),axis=0).item()/len(foreground_index_list))*100\n",
    "    \n",
    "    ftpf  = (np.sum(np.logical_and(predicted_indexes == foreground_index_list,\n",
    "                                 prediction_list != labels_list),axis=0).item()/len(foreground_index_list))*100\n",
    "    \n",
    "    ffpf  = (np.sum(np.logical_and(predicted_indexes != foreground_index_list,\n",
    "                                 prediction_list != labels_list),axis=0).item()/len(foreground_index_list))*100\n",
    "#     focus_true = (np.sum(predicted_indexes == foreground_index_list,axis=0).item()/\n",
    "#                         len(foreground_index_list))*100\n",
    "    accuracy = (np.sum(prediction_list == labels_list,axis=0)/len(labels_list) )*100\n",
    "    \n",
    "    return ftpt,ffpt,ftpf,ffpf,accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method 3\n",
    "def evaluation_method_3(dataloader,focus,classification):\n",
    "    \"\"\"\n",
    "    returns \\sum_j( \\sigma_k(\\alpha_j * g(x_j)) \n",
    "    \"\"\"\n",
    "    \n",
    "    predicted_indexes = []\n",
    "    foreground_index_list = []\n",
    "    prediction_list = []\n",
    "    labels_list = []\n",
    "    focus.eval()\n",
    "    classification.eval()\n",
    "    with torch.no_grad():\n",
    "        for j,data in enumerate(dataloader):\n",
    "            images,labels,foreground_index = data\n",
    "            images = images.float()\n",
    "            images = images.to(device)\n",
    "            foreground_index_list.append(foreground_index.numpy())\n",
    "            labels_list.append(labels.numpy())\n",
    "            batch = images.size(0)\n",
    "            scores,features = focus(images)\n",
    "            alphas = F.softmax(scores,dim=1)\n",
    "            if len(scores.shape)>2:\n",
    "                indexes = torch.argmax(F.softmax(scores,dim=1),dim=1).cpu().numpy()[:,0]\n",
    "            else:\n",
    "                indexes = torch.argmax(F.softmax(scores,dim=1),dim=1).cpu().numpy()\n",
    "            predicted_indexes.append(indexes)\n",
    "            features = features.reshape(batch,patches,256,3,3)\n",
    "            if len(images.shape)>3:\n",
    "                features = torch.sum(alphas[:,:,None,None,None]*features,dim=1)\n",
    "            else:\n",
    "                images = torch.sum(alphas*images,dim=1)\n",
    "            \n",
    "            outputs = F.softmax(classification(features),dim=1)\n",
    "            prediction = torch.argmax(outputs,dim=1)\n",
    "            prediction_list.append(prediction.cpu().numpy())\n",
    "#     print(len(predicted_indexes))\n",
    "    predicted_indexes = np.concatenate(predicted_indexes,axis=0)\n",
    "    foreground_index_list = np.concatenate(foreground_index_list,axis=0)\n",
    "    prediction_list = np.concatenate(prediction_list,axis=0)\n",
    "    labels_list = np.concatenate(labels_list,axis=0)\n",
    "\n",
    "\n",
    "    ftpt = (np.sum(np.logical_and(predicted_indexes == foreground_index_list,\n",
    "                                 prediction_list == labels_list),axis=0).item()/len(foreground_index_list))*100\n",
    "    \n",
    "    ffpt  = (np.sum(np.logical_and(predicted_indexes != foreground_index_list,\n",
    "                                 prediction_list == labels_list),axis=0).item()/len(foreground_index_list))*100\n",
    "    \n",
    "    ftpf  = (np.sum(np.logical_and(predicted_indexes == foreground_index_list,\n",
    "                                 prediction_list != labels_list),axis=0).item()/len(foreground_index_list))*100\n",
    "    \n",
    "    ffpf  = (np.sum(np.logical_and(predicted_indexes != foreground_index_list,\n",
    "                                 prediction_list != labels_list),axis=0).item()/len(foreground_index_list))*100\n",
    "#     focus_true = (np.sum(predicted_indexes == foreground_index_list,axis=0).item()/\n",
    "#                         len(foreground_index_list))*100\n",
    "    accuracy = (np.sum(prediction_list == labels_list,axis=0)/len(labels_list) )*100\n",
    "    \n",
    "    return ftpt,ffpt,ftpf,ffpf,accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "wandb version 0.12.18 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.17"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/malt/Documents/Notebooks/Hard_Attention/wandb/run-20220620_152910-2jbdx81k</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/malt-lab/Interpretability_CIFAR_10_Experiments/runs/2jbdx81k\" target=\"_blank\">HA_sixth_layer_averaging</a></strong> to <a href=\"https://wandb.ai/malt-lab/Interpretability_CIFAR_10_Experiments\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nos_epochs = 50\n",
    "\n",
    "learning_rates = [0.0001] #0.0025\n",
    "\n",
    "run =  wandb.init(project=\"Interpretability_CIFAR_10_Experiments\",\n",
    "               name =\"HA_sixth_layer_averaging\",config = {\n",
    "                   \"learning rate \":learning_rates,\n",
    "                   \"epochs\":50\n",
    "               },save_code=True)\n",
    "\n",
    "\n",
    "focus_lr_plots = []\n",
    "classification_lr_plots = []\n",
    "n_seeds = [0,1,2]\n",
    "for seed in n_seeds:\n",
    "    for run_no in range(len(learning_rates)):\n",
    "        torch.manual_seed(seed)\n",
    "        focus = Focus_6()\n",
    "        focus = focus.to(device)\n",
    "\n",
    "\n",
    "\n",
    "        torch.manual_seed(seed)\n",
    "        classification = Classification_6()\n",
    "        classification = classification.to(device)\n",
    "\n",
    "\n",
    "        lr = learning_rates[run_no] \n",
    "\n",
    "        Criterion = nn.CrossEntropyLoss(reduction=\"none\") #nn.BCELoss(reduction=\"none\")\n",
    "        focus_optimizer = optim.Adam(focus.parameters(), lr=lr)#,momentum=0.9)\n",
    "        classification_optimizer = optim.Adam(classification.parameters(),lr=lr)#,momentum=0.9)\n",
    "\n",
    "\n",
    "        loss_list = []\n",
    "\n",
    "\n",
    "        for epoch in tqdm(range(nos_epochs)):\n",
    "            focus.train()\n",
    "            classification.train()\n",
    "\n",
    "            epoch_loss = [] \n",
    "\n",
    "            for i,data in enumerate(train_loader):\n",
    "                focus,classification,focus_optimizer,classification_optimizer=train_model1(data,\n",
    "                                                                                          focus,\n",
    "                                                                                          classification,\n",
    "                                                                                          focus_optimizer,\n",
    "                                                                                          classification_optimizer,\n",
    "                                                                                          Criterion)\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    images,labels,fore_idx = data\n",
    "                    batch = images.size(0)\n",
    "                    patches = images.size(1)\n",
    "                    images,labels = images.to(device),labels.to(device)\n",
    "                    alpha,features = focus(images)\n",
    "\n",
    "                    alphas = torch.softmax(alpha,dim=1)\n",
    "\n",
    "                    features =  features.reshape(batch*patches,256,3,3)\n",
    "                    outputs = classification(features)\n",
    "                    loss = my_cross_entropy(outputs,labels,alphas,Criterion)\n",
    "\n",
    "                epoch_loss.append(loss.item())\n",
    "            #print('[%d] loss: %.3f' %(epoch+1,np.mean(epoch_loss)))\n",
    "            wandb.log(\n",
    "                    {\"loss_lr_\"+str(learning_rates[run_no]):np.mean(epoch_loss).item(),\n",
    "                    \"Epoch\":epoch+1})\n",
    "            wandb.log({\"Epoch\":epoch+1})\n",
    "\n",
    "            loss_list.append(np.mean(epoch_loss))\n",
    "\n",
    "    print_analysis(train_loader,focus,classification,dataset=\"training\")\n",
    "    print_analysis(test_loader,focus,classification,dataset=\"testing\")\n",
    "    calculate_metrics(focus,classification,train_loader,\"training\")\n",
    "    calculate_metrics(focus,classification,test_loader,\"testing\")\n",
    "    # print_analysis(test_data_loader,focus,classification,dataset=\"cifar test\")    \n",
    "    print(\"Finished Training\")\n",
    "    torch.save(focus.state_dict(), 'focus.pth')  \n",
    "    torch.save(classification.state_dict(), 'classification.pth')  \n",
    "    artifact = wandb.Artifact('model', type='model')\n",
    "    artifact.add_file('focus.pth')\n",
    "    artifact.add_file('classification.pth')\n",
    "    run.log_artifact(artifact)\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
